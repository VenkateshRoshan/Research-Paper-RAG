{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prediction Results ===\n",
      "\n",
      "Query: Give me an idea on Transformers\n",
      "\n",
      "Response: A comprehensive survey of techniques for optimizing transformer inference.\n",
      "\n",
      "References:\n",
      "\n",
      "[1] An Introduction to Transformers\n",
      "    Authors: Richard E. Turner\n",
      "    Categories: cs.LG cs.AI\n",
      "    Relevance Score: 0.48\n",
      "\n",
      "[2] A Survey of Techniques for Optimizing Transformer Inference\n",
      "    Authors: Krishna Teja Chitty-Venkata, Sparsh Mittal, Murali Emani, Venkatram\n",
      "  Vishwanath, Arun K. Somani\n",
      "    Categories: cs.LG cs.AR cs.CL cs.CV\n",
      "    Relevance Score: 0.46\n",
      "\n",
      "Timestamp: 2024-12-20T23:13:03.616904\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.auth import credentials, load_credentials_from_dict\n",
    "import json\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from datetime import datetime\n",
    "\n",
    "# Load credentials\n",
    "credentials, project_id = load_credentials_from_dict(\n",
    "    json.load(open('research-paper-rag-0a8819b735b9.json'))\n",
    ")\n",
    "\n",
    "# Initialize client\n",
    "client_options = {\"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"}\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "    client_options=client_options,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "# Endpoint configuration\n",
    "project = \"research-paper-rag\"\n",
    "location = \"us-central1\"\n",
    "endpoint_id = \"3729166308927864832\"\n",
    "endpoint = f\"projects/{project}/locations/{location}/endpoints/{endpoint_id}\"\n",
    "\n",
    "# Input data\n",
    "input_data = {\n",
    "    \"instances\": [{\n",
    "        \"query\": \"Give me an idea on Transformers\",\n",
    "        'max_tokens': 500,\n",
    "        'num_papers': 2\n",
    "    }]\n",
    "}\n",
    "\n",
    "def format_references(references):\n",
    "    \"\"\"Format the references list from MapComposite objects\"\"\"\n",
    "    formatted_refs = []\n",
    "    for ref in references:\n",
    "        ref_dict = dict(ref)\n",
    "        formatted_refs.append({\n",
    "            'title': ref_dict.get('title', ''),\n",
    "            'authors': list(ref_dict.get('authors', [])),\n",
    "            'categories': ref_dict.get('categories', ''),\n",
    "            'relevance_score': ref_dict.get('relevance_score', 0.0),\n",
    "            'citation': ref_dict.get('citation', '')\n",
    "        })\n",
    "    return formatted_refs\n",
    "\n",
    "try:\n",
    "    # Send prediction request\n",
    "    response = client.predict(endpoint=endpoint, instances=input_data[\"instances\"])\n",
    "    \n",
    "    # Get the first prediction\n",
    "    prediction = dict(response.predictions[0])\n",
    "    \n",
    "    # Format the output\n",
    "    formatted_output = {\n",
    "        'query': prediction['query'],\n",
    "        'response': prediction['response'],\n",
    "        'references': format_references(prediction['references']),\n",
    "        'metadata': dict(prediction['metadata']),\n",
    "        'timestamp': prediction['timestamp']\n",
    "    }\n",
    "    \n",
    "    # Pretty print the results\n",
    "    print(\"\\n=== Prediction Results ===\\n\")\n",
    "    print(f\"Query: {formatted_output['query']}\\n\")\n",
    "    print(f\"Response: {formatted_output['response']}\\n\")\n",
    "    print(\"References:\")\n",
    "    for i, ref in enumerate(formatted_output['references'], 1):\n",
    "        print(f\"\\n[{i}] {ref['title']}\")\n",
    "        print(f\"    Authors: {', '.join(ref['authors'])}\")\n",
    "        print(f\"    Categories: {ref['categories']}\")\n",
    "        print(f\"    Relevance Score: {ref['relevance_score']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nTimestamp: {formatted_output['timestamp']}\")\n",
    "    \n",
    "    # Optionally, save the full response to a file\n",
    "    with open('prediction_response.json', 'w') as f:\n",
    "        json.dump(formatted_output, f, indent=2)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during prediction: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "PREDICTION RESULTS\n",
      "================================================================================\n",
      "\n",
      "QUERY:\n",
      "what is Transformer in Deep Learning?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "A neural network component that can be used to learn useful representations of\n",
      "sequences or sets of data-points.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "REFERENCES:\n",
      "\n",
      "[1] Title: An Introduction to Transformers\n",
      "    Authors: Richard E. Turner\n",
      "    Categories: cs.LG cs.AI\n",
      "    Relevance Score: 0.64\n",
      "    Citation: Richard E. Turner. \"An Introduction to Transformers\". cs.LG cs.AI.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Metadata:\n",
      "    model: google/flan-t5-base\n",
      "    style: academic\n",
      "    max_tokens: 500.0\n",
      "    num_papers: 1.0\n",
      "\n",
      "Timestamp: 2024-12-20T23:20:26.841521\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Full response saved to 'prediction_response.json'\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.auth import credentials, load_credentials_from_dict\n",
    "import json\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "# Load credentials\n",
    "credentials, project_id = load_credentials_from_dict(\n",
    "    json.load(open('research-paper-rag-0a8819b735b9.json'))\n",
    ")\n",
    "\n",
    "# Initialize client\n",
    "client_options = {\"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"}\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "    client_options=client_options,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "# Endpoint configuration\n",
    "project = \"research-paper-rag\"\n",
    "location = \"us-central1\"\n",
    "endpoint_id = \"3729166308927864832\"\n",
    "endpoint = f\"projects/{project}/locations/{location}/endpoints/{endpoint_id}\"\n",
    "\n",
    "# Input data\n",
    "input_data = {\n",
    "    \"instances\": [{\n",
    "        \"query\": \"what is Transformer in Deep Learning?\",\n",
    "        'max_tokens': 500,  # Increased max tokens\n",
    "        'num_papers': 1     # Increased number of papers\n",
    "    }]\n",
    "}\n",
    "\n",
    "def format_references(references):\n",
    "    \"\"\"Format the references list from MapComposite objects\"\"\"\n",
    "    formatted_refs = []\n",
    "    for ref in references:\n",
    "        ref_dict = dict(ref)\n",
    "        formatted_refs.append({\n",
    "            'title': ref_dict.get('title', ''),\n",
    "            'authors': list(ref_dict.get('authors', [])),\n",
    "            'categories': ref_dict.get('categories', ''),\n",
    "            'relevance_score': ref_dict.get('relevance_score', 0.0),\n",
    "            'citation': ref_dict.get('citation', '')\n",
    "        })\n",
    "    return formatted_refs\n",
    "\n",
    "def print_wrapped_text(label, text, width=80):\n",
    "    \"\"\"Print text with proper wrapping\"\"\"\n",
    "    print(f\"\\n{label}:\")\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "    print(wrapped_text)\n",
    "\n",
    "try:\n",
    "    # Send prediction request\n",
    "    response = client.predict(endpoint=endpoint, instances=input_data[\"instances\"])\n",
    "    \n",
    "    # Get the first prediction\n",
    "    prediction = dict(response.predictions[0])\n",
    "    \n",
    "    # Format the output\n",
    "    formatted_output = {\n",
    "        'query': prediction['query'],\n",
    "        'response': prediction['response'],\n",
    "        'references': format_references(prediction['references']),\n",
    "        'metadata': dict(prediction['metadata']),\n",
    "        'timestamp': prediction['timestamp']\n",
    "    }\n",
    "    \n",
    "    # Print complete results with proper formatting\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nPREDICTION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print_wrapped_text(\"QUERY\", formatted_output['query'])\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    print_wrapped_text(\"RESPONSE\", formatted_output['response'])\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"\\nREFERENCES:\")\n",
    "    for i, ref in enumerate(formatted_output['references'], 1):\n",
    "        print(f\"\\n[{i}] Title: {ref['title']}\")\n",
    "        print(f\"    Authors: {', '.join(ref['authors'])}\")\n",
    "        print(f\"    Categories: {ref['categories']}\")\n",
    "        print(f\"    Relevance Score: {ref['relevance_score']:.2f}\")\n",
    "        print(f\"    Citation: {ref['citation']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"\\nMetadata:\")\n",
    "    for key, value in formatted_output['metadata'].items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nTimestamp: {formatted_output['timestamp']}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Save complete response to file\n",
    "    with open('prediction_response.json', 'w') as f:\n",
    "        json.dump(formatted_output, f, indent=2)\n",
    "    print(\"\\nFull response saved to 'prediction_response.json'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during prediction: {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.auth import credentials, load_credentials_from_dict\n",
    "import json\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from datetime import datetime\n",
    "\n",
    "# Load credentials and initialize client\n",
    "credentials, project_id = load_credentials_from_dict(\n",
    "    json.load(open('research-paper-rag-0a8819b735b9.json'))\n",
    ")\n",
    "\n",
    "client_options = {\"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"}\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "    client_options=client_options,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "# Endpoint configuration\n",
    "project = \"research-paper-rag\"\n",
    "location = \"us-central1\"\n",
    "endpoint_id = \"3729166308927864832\"\n",
    "endpoint = f\"projects/{project}/locations/{location}/endpoints/{endpoint_id}\"\n",
    "\n",
    "def format_references(references):\n",
    "    \"\"\"Format references into a readable string\"\"\"\n",
    "    formatted_text = \"\"\n",
    "    for i, ref in enumerate(references, 1):\n",
    "        ref_dict = dict(ref)\n",
    "        formatted_text += f\"\\n[{i}] Title: {ref_dict.get('title', '')}\\n\"\n",
    "        formatted_text += f\"    Authors: {', '.join(ref_dict.get('authors', []))}\\n\"\n",
    "        formatted_text += f\"    Categories: {ref_dict.get('categories', '')}\\n\"\n",
    "        formatted_text += f\"    Relevance Score: {ref_dict.get('relevance_score', 0.0):.2f}\\n\"\n",
    "        formatted_text += f\"    Citation: {ref_dict.get('citation', '')}\\n\\n\"\n",
    "    return formatted_text\n",
    "\n",
    "def predict_query(query, max_tokens):\n",
    "    try:\n",
    "        # Prepare input data\n",
    "        input_data = {\n",
    "            \"instances\": [{\n",
    "                \"query\": query,\n",
    "                'max_tokens': max_tokens,\n",
    "                'num_papers': 2\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        # Get prediction\n",
    "        response = client.predict(endpoint=endpoint, instances=input_data[\"instances\"])\n",
    "        prediction = dict(response.predictions[0])\n",
    "        \n",
    "        # Format response\n",
    "        main_response = prediction['response']\n",
    "        references = format_references(prediction['references'])\n",
    "        \n",
    "        return main_response, references\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"Error retrieving references\"\n",
    "    \n",
    "predict_query(\"What is the Transformer model?\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'no'}]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "# inference\n",
    "output = pipe(\"what is Transformers and does it useful ?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
